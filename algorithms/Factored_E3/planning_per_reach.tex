\paragraph{One policy per state variable}
\label{sec:one_policy_per_state_variable}

For some MDPs it is possible to compute a separate policy for each state
variable individually. This is the case when there is a separate action taken
for each state variable, which is true for the Invasive Species environment
(section \ref{sec:experiment_env}). In the implementation of \etre\ used in
this thesis, this policy computation is performed in two steps. 

In the first step, a policy is calculated for state variables that have no
other state variables than themselves as parents in the DBN, and these states
are marked as done. This calculation is done by value iteration where the reward function
is described in equation \eqref{equation:fusk}. Since there is now a decided action for each value for
these state variables, the transition probabilities for these variables can be
considered as pure Markov chains in the next step. In this step, a policy is
found for state variables whose parents are marked as done, until all state
variables are done. In this second step, the transition probabilities of the
parents are thus treated as independent of the action taken.

The reward function for the partial action $a(i)$ and the partial state $s(i)$ in the Invasive Species environment can be described as follows:

\begin{align}
\label{equation:fusk}
R(s(i),a(i)) = \, 
 & c(s(i)) r_c + t(s(i)) r_t  \nonumber \\
 & + n(s(i)) r_n + e(s(i)) r_e   \nonumber \\
 & +  x(a(i)) t(s(i)) r_x + p(a(i)) e(s(i)) r_p
\end{align}

where \\
$c(s(i))$ is 1 if $s(i)$ is infected, 0 otherwise. \\
$x(a(i))$ is 1 if action was taken to exterminate tamarix trees, 0 otherwise. \\
$p(a(i))$ is 1 if action was taken to plant native trees, 0 otherwise. \\
$t(s(i))$, $n(s(i))$, $e(s(i))$ is, in $s(i)$, the number of tamarix-infested habitats, habitats with native trees and empty habitats, respectively. \\
$r_c$, $r_t$, $r_n$, $e_r$, $r_x$ and $r_p$ are rewards given for each infected reach, tamarix-invaded habitat, native habitat, empty habitats, extermination of tamarix tree and restoration of native tree in empty slot, respectively.

Since equation \eqref{equation:fusk} is a simple linear equation, the unknown variables ($r_i, r_t, r_x$ and $r_p$) can be calculated exactly once a few data points have been collected. Once this is done, the agent can use equation \eqref{equation:fusk} to calculate the reward for any partial state-action pair. 

Planning for each state variable individually has the benefit of making the
planning algorithm linear in the number of state variables, greatly reducing
the time needed to calculate a policy. However, there are several downsides to
using this kind of approximation, some of which are discussed in section
\ref{sec:e3_factored_discussion}. 
