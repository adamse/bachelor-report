\subsection{Value iteration with confidence intervals}
\label{sec:modification_conf_interval}
When Compute$\tilde{P}$ has been run, the resulting $\tilde{P}$ is used in a standard value
iteration, as in \eqref{equation:q_upper}. Thus, Compute$\tilde{P}$ finds the probability distribution that maximizes the sum in this equation. 

Finally, optimistic state values are computed according to \eqref{equation:vMBIE}. These values are simply the value of the best action for each state. These values are used the next time that Compute$\tilde{P}$ is run and ``good'' and ``bad'' states need to be found. 

\begin{comment}
The confidence  bounds on the Q-values in the MBIE-algorithm are calculated by
making a maximally optimistic estimation of these values, given some confidence
parameter. The less times a state-action pair has been visited, the more
optimistic this estimation will be. This has the effect of promoting
exploration of actions that have been taken few times. 

When a state is first encountered by the agent, the Q-values associated with
the state are initialized with the maximum achievable reward. When the actions
are later performed, the state-action pairs have their Q-values gradually
decreased depending on the expected value. Given time, the confidence bounds will
become smaller and smaller, and the policy will converge to optimal actions
with confidence specified by a confidence parameter. The bound for the
confidence interval on a Q-value can be calculated by iterating the following
equation (cf. section~\ref{sec:valueiteration} about the basic value iteration
algorithm) for all state-action pairs until it converges:
\end{comment}

\begin{align}
\label{equation:q_upper}
Q_{upper} (s, a) = & R(s, a) + \nonumber \\
& \operatorname*{max}_{\tilde{P}(s, a)\in CI(P(s, a), \delta)} \gamma \sum_{s'} \tilde{P}(s'|s, a)\operatorname*{max}_{a'} Q_{upper}(s', a')
\end{align}

\begin{equation}
\label{equation:vMBIE}
V_{upper}(s) = \operatorname*{max}_aQ_{upper}(s,a)
\end{equation}
