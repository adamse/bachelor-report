\subsection{Value iteration with confidence intervals}
\label{sec:modification_conf_interval}

The confidence  bounds on the Q-values in the MBIE-algorithm are calculated by
making a maximally optimistic estimation of these values, given some confidence
parameter. The less times a state-action pair has been visited, the more
optimistic this estimation will be. This has the effect of promoting
exploration of actions that have been taken few times. 

When a state is first encountered by the agent, the Q-values associated with
the state are initialized with the maximum achievable reward. When the actions
are later performed, the state-action pairs have their Q-values gradually
decreased depending the expected value. Given time, the confidence bounds will
become smaller and smaller, and the policy will converge to optimal actions
with confidence specified by a confidence parameter. The bound for the
confidence interval on a Q-value can be calculated by iterating the following
equation (cf. section~\ref{sec:valueiteration} about the basic value iteration
algorithm) for all state-action pairs until it converges:

\begin{align}
\label{equation:q_upper}
Q_{upper} (s, a) = & R(s, a) + \nonumber \\
& \operatorname*{max}_{\tilde{P}(s, a)\in CI(P(s, a), \delta_1)} \gamma \sum_{s'} \tilde{P}(s'|s, a)\operatorname*{max}_{a'} Q_{upper}(s', a')
\end{align}


To efficiently calculate the correct expected maximum Q-values in equation \eqref{equation:q_upper}, a method is used that generates a distribution of the transition probabilities $\tilde{P}$ that maximizes the sum in the equation \parencite{Strehl20081309}. This algorithm is referred to as Compute$\tilde{P}$ in this report.


\subsection{Compute$\tilde{P}$, optimistic estimations of transition probabilities}

The fundamental idea of the Compute$\tilde{P}$ method is that it starts with the observed transition probabilities, $\hat{P}$, and then it moves probability mass from ``bad'' outcomes to ``good'' outcomes and finall returns an updated version of $\tilde{P}$. This updated $\tilde{P}$ maximizes the sum in equation \ref{equation:q_upper}, which has been proved by Strehl and Littman \parencite{Strehl20081309}.

The state transition probability distribution is initialized according to \ref{equation:roof}, which corresponds to the observed probabilities.

\begin{equation}
\label{equation:roof}
\tilde{P} := \hat{P} = \frac{N(s,a,s')}{N(s,a)}
\end{equation}

In this equation $N(s, a)$ is the number of times action $a$ has been tried in state $s$ and $N(s, a, s')$ is the number of times action $a$ has been taken in state $s$ and this resulted in the agent ending up in state $s'$.

The procedure of moving probabilities is done by first finding the outcome state with the best V-value, calling it $\overline{s}$. Next, analogously, the outcome with the worst V-value with an observed probability of greater than 0 is found. This state is called $\underline{s}$. 

The probability values $\tilde{P}(\underline{s}|s,a)$ and $\tilde{P}(\overline{s}|s,a)$ are then increased/decreased by $\xi$, the maximum amount they are allowed to change, according to equations \eqref{equation:ptilde_floor} and \eqref{equation:ptilde_roof}.


\begin{equation}
\label{equation:ptilde_floor}
\tilde{P}(\underline{s}|s,a) := \tilde{P}(\underline{s}|s,a)-\xi
\end{equation}

\begin{equation}
\label{equation:ptilde_roof}
\tilde{P}(\overline{s}|s,a) := \tilde{P}(\overline{s}|s,a)+\xi
\end{equation}

Since we need to ensure that the sum of the probabilities sum to one and that no single transition probability falls below zero or exceeds one we are only allowed to modify the probability distribution by at most $\xi$, as given by equation \ref{equation:xi}. 

\begin{equation}
\label{equation:xi}
\xi = \min\{
  1 - \tilde{P}(\overline{s} | s, a)
  , \tilde{P}(\underline{s} | s, a)
  , \Delta \omega 
\}
\end{equation}

Here, $\Delta \omega$ is $\frac{\sqrt{\frac{2|ln(2^{|S|}-2) - ln  \delta |}{N(s,a)}}}{2}$, where $|S|$ is the total number of states. $\Delta \omega$ denotes the total probability mass to be moved. If $\xi$ is less than $\Delta \omega$, new states $\overline{s}$ and $\underline{s}$ are found, and probabilities moved until mass equal to $\Delta \omega$ has been moved in total. 

The confidence interval $CI(\hat{P} | N(s, a), \delta_1)$ denotes a set of probability distributions where the probability is $1 - \delta$ for each element in that set where the confidence interval is within distance of $\omega(N(s,a),\delta)$ of the maximum likelihood estimate for P \parencite{dietterich2013pac}. This means that with probability $1-\delta$, the actual transition probabilties are between the observed probabilities and the probabilities returned by Compute $\tilde{P}$   \todo[inline]{Har jag rätt här med sista meningen? /Daniel}

\begin{equation}
\label{equation:balls_of_steels}
CI(\hat{P} | N(s, a), \delta_1)  = \left\{\tilde{P} \left| \|\tilde{P} - \hat{P}\|_1 \le \omega(N(s,a), \delta)\right.\right\}
\end{equation}


%\begin{equation}
%\label{equation:N}
%N(s,a) = \sum_{s'}{N(s,a,s')}
%\end{equation}

%\begin{equation}
%\label{equation:ptilde_init}
%\tilde{P}(s'|s,a) := \hat{P}(s'|s,a)
%\end{equation}

%\begin{equation}
%\label{equation:deltaomega}
%\Delta\omega = \omega(N(s,a), \delta)/2
%\end{equation}

\label{goto}


\ifx false
\label{sec:agent:step}
\begin{algorithm}      
\caption{Pseudo code for algorithm}          
\begin{algorithmic}
\Loop
\State action := getActionFromPolicy(s')
\State sample(s, a, s', r) := takeActionInState(action) 
\State saveSampleInModel(sample)
\If{the pair s a is new}
\State calculateNewPolicy()
\EndIf
\If{lastPolicyUpdate was more than X samples ago}
\State calculateNewPolicy()
\EndIf
\EndLoop
\end{algorithmic}
\end{algorithm}
\fi

