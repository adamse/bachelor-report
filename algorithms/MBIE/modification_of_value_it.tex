
\subsection{Optimistic estimations of transition probabilities}
\label{sec:computep}

The first step of the MBIE algorithm is to find optimistic estimations of transition probabilities. Equation \eqref{equation:balls_of_steels} describes, as a set, the confidence interval (CI) used by MBIE for the probability distribution over destination states when taking action $a$ in state $s$. So, the first task of the algorithm is to find the element within this set that is maximally optimistic, meaning that it gives the best value to $(s,a)$ in the following value-iteration step of the MBIE algorithm. 

In equation \eqref{equation:balls_of_steels}, $\hat{P}$ is the observed probability distribution (treated as a vector) for destination states from $(s,a)$, $N(s,a)$ is the number of times that the state-action pair $(s,a)$ has been observed, $\delta$ is a confidence parameter,  $\omega$ is a value given by \eqref{equation:omega} and $\|x\|_1$ denotes the l1-norm of the vector $x$. The l1-norm is the sum of the absolute value of all elements of a vector. 

\begin{equation}
\label{equation:balls_of_steels}
CI\left(\hat{P} \left| N(s, a), \delta\right.\right)  = \left\{\tilde{P} \left| \|\tilde{P} - \hat{P}\|_1 \le \omega(N(s,a), \delta)\right.\right\}
\end{equation}

\begin{equation}
\label{equation:omega}
   \omega(N(s,a),\delta) = {\sqrt{\frac{2|ln(2^{|S|}-2) - ln  \delta |}{N(s,a)}}}
\end{equation}

In equation \eqref{equation:omega}, $\omega(N(s,a),\delta)$, gives a bound for how much the vector of transition probabilities can be changed from the observed values, while remaining within the confidence interval. In this equation, $|S|$ is the number of states in the MDP and the other variables have the same meaning as in \eqref{equation:balls_of_steels}. For the derivation of this formula, see  \textcite{Strehl20081309}.

\subsection{Compute$\tilde{P}$}
The method for finding the sought element within the set denoted by \eqref{equation:balls_of_steels} 
is refered to as Compute$\tilde{P}$ in this report. 
The fundamental idea of the Compute$\tilde{P}$ method is that it starts with
the observed transition probabilities $\hat{P}$ and then it moves probability
mass from ``bad'' outcomes to ``good'' outcomes and finally returns the resulting probability distribution, $\tilde{P}$. 


\paragraph{Initialization} The state transition probability distribution is initialized according to
\eqref{equation:roof}, which corresponds to the observed probabilities.

\begin{equation}
\label{equation:roof}
\tilde{P}(s'|s, a) := \hat{P}(s'|s, a) = \frac{N(s,a,s')}{N(s,a)}
\end{equation}

In \eqref{equation:roof}, $N(s, a)$ is the number of times action $a$ has been taken in
state $s$ and $N(s, a, s')$ is the number of times action $a$ has been taken in
state $s$ and the agent ended up in state $s'$.

\paragraph{Moving probability mass} The procedure of moving probability mass is done by first finding the outcome
state with the best $V$-value and observed probability less than 1, calling it
$\overline{s}$. Analogously the outcome with the worst $V$-value with an
observed probability of greater than 0 is found, and this state is called
$\underline{s}$. If a $V$-value has not been computed yet for a certain state, it is 
assumed to have the maximum possible value. 

The probability values $\tilde{P}(\underline{s}|s,a)$ and
$\tilde{P}(\overline{s}|s,a)$ are then increased or decreased according to
equations \eqref{equation:ptilde_floor} and \eqref{equation:ptilde_roof}.

\begin{equation}
\label{equation:ptilde_floor}
\tilde{P}(\underline{s}|s,a) := \tilde{P}(\underline{s}|s,a)-\xi
\end{equation}

\begin{equation}
\label{equation:ptilde_roof}
\tilde{P}(\overline{s}|s,a) := \tilde{P}(\overline{s}|s,a)+\xi
\end{equation}

Since the sum of the probabilities needs to equal one and 
no single transition probability may fall below zero or exceed one, the probability distribution can only be modified by at most $\xi$, as given by
equation \eqref{equation:xi}, where $\Delta\omega = \omega / 2$. The variable $\Delta\omega$ denotes the total mass that can be moved, without $\tilde{P}$ having a lower chance than $1 - \delta$ of being within the confidence interval for the probability distribution. If $\xi$ is less than $\Delta \omega$, new states
$\overline{s}$ and $\underline{s}$ are found, and probabilities moved until
mass equal to $\Delta \omega$ has been moved in total or the probability mass has all been moved to an optimal state. 

\begin{equation}
\label{equation:xi}
\xi = \min\{
  1 - \tilde{P}(\overline{s} | s, a)
  , \tilde{P}(\underline{s} | s, a)
  , \Delta \omega 
\}
\end{equation}


\label{goto}



\subsection{Value iteration with confidence intervals}
\label{sec:modification_conf_interval}
When Compute$\tilde{P}$ has been run, the resulting $\tilde{P}$ is used in a standard value
iteration, as in \eqref{equation:q_upper}. Thus, Compute$\tilde{P}$ finds the probability distribution that maximizes the sum in this equation. 

Finally, optimistic state values are computed according to \eqref{equation:vMBIE}. These values are simply the value of the best action for each state. These values are used the next time that Compute$\tilde{P}$ is run and ``good'' and ``bad'' states need to be found. 

\begin{comment}
The confidence  bounds on the Q-values in the MBIE-algorithm are calculated by
making a maximally optimistic estimation of these values, given some confidence
parameter. The less times a state-action pair has been visited, the more
optimistic this estimation will be. This has the effect of promoting
exploration of actions that have been taken few times. 

When a state is first encountered by the agent, the Q-values associated with
the state are initialized with the maximum achievable reward. When the actions
are later performed, the state-action pairs have their Q-values gradually
decreased depending on the expected value. Given time, the confidence bounds will
become smaller and smaller, and the policy will converge to optimal actions
with confidence specified by a confidence parameter. The bound for the
confidence interval on a Q-value can be calculated by iterating the following
equation (cf. section~\ref{sec:valueiteration} about the basic value iteration
algorithm) for all state-action pairs until it converges:
\end{comment}

\begin{align}
\label{equation:q_upper}
Q_{upper} (s, a) = & R(s, a) + \nonumber \\
& \operatorname*{max}_{\tilde{P}(s, a)\in CI(P(s, a), \delta)} \gamma \sum_{s'} \tilde{P}(s'|s, a)\operatorname*{max}_{a'} Q_{upper}(s', a')
\end{align}


\begin{equation}
\label{equation:vMBIE}
V_{upper}(s) = max_aQ_{upper}(s,a)
\end{equation}
