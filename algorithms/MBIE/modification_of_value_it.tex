\subsection{Value iteration with confidence intervals}
\label{sec:modification_conf_interval}

The confidence  bounds on the Q-values in the MBIE-algorithm are calculated by
making a maximally optimistic estimation of these values, given some confidence
parameter. The less times a state-action pair has been visited, the more
optimistic this estimation will be. This has the effect of promoting
exploration of actions that have been taken few times. 

When a state is first encountered by the agent, the Q-values associated with
the state are initialized with the maximum achievable reward. When the actions
are later performed, the state-action pairs have their Q-values gradually
decreased depending on the expected value. Given time, the confidence bounds will
become smaller and smaller, and the policy will converge to optimal actions
with confidence specified by a confidence parameter. The bound for the
confidence interval on a Q-value can be calculated by iterating the following
equation (cf. section~\ref{sec:valueiteration} about the basic value iteration
algorithm) for all state-action pairs until it converges:

\begin{align}
\label{equation:q_upper}
Q_{upper} (s, a) = & R(s, a) + \nonumber \\
& \operatorname*{max}_{\tilde{P}(s, a)\in CI(P(s, a), \delta_1)} \gamma \sum_{s'} \tilde{P}(s'|s, a)\operatorname*{max}_{a'} Q_{upper}(s', a')
\end{align}

\begin{equation}
\label{equation:balls_of_steels}
CI\left(\hat{P} \left| N(s, a), \delta_1\right.\right)  = \left\{\tilde{P} \left| \|\tilde{P} - \hat{P}\|_1 \le \omega(N(s,a), \delta)\right.\right\}
\end{equation}

To efficiently calculate the correct expected maximum Q-values in
equation~\eqref{equation:q_upper}, a method is used that generates a
distribution of the transition probabilities $\tilde{P}$ that maximizes the sum
in the equation \parencite{Strehl20081309}. This algorithm is referred to as
Compute$\tilde{P}$ in this report.

\subsection{Compute$\tilde{P}$, optimistic estimations of transition probabilities}

The fundamental idea of the Compute$\tilde{P}$ method is that it starts with
the observed transition probabilities $\hat{P}$ and then it moves probability
mass from ``bad'' outcomes to ``good'' outcomes and finally returns an updated
version of $\tilde{P}$. This updated $\tilde{P}$ maximizes the sum in equation
\eqref{equation:q_upper}, which has been proved in \textcite{Strehl20081309}.
The state transition probability distribution is initialized according to
\eqref{equation:roof}, which corresponds to the observed probabilities.

\begin{equation}
\label{equation:roof}
\tilde{P} := \hat{P} = \frac{N(s,a,s')}{N(s,a)}
\end{equation}

In \eqref{equation:roof}, $N(s, a)$ is the number of times action $a$ has been taken in
state $s$ and $N(s, a, s')$ is the number of times action $a$ has been taken in
state $s$ and the agent ended up in state $s'$.

The procedure of moving probabilities is done by first finding the outcome
state with the best $V$-value and observed probabilty less than 1, calling it
$\overline{s}$. Analogously the outcome with the worst $V$-value with an
observed probability of greater than 0 is found, this state is called
$\underline{s}$. 

The probability values $\tilde{P}(\underline{s}|s,a)$ and
$\tilde{P}(\overline{s}|s,a)$ are then increased or decreased according to
equations \eqref{equation:ptilde_floor} and \eqref{equation:ptilde_roof}.

\begin{equation}
\label{equation:ptilde_floor}
\tilde{P}(\underline{s}|s,a) := \tilde{P}(\underline{s}|s,a)-\xi
\end{equation}

\begin{equation}
\label{equation:ptilde_roof}
\tilde{P}(\overline{s}|s,a) := \tilde{P}(\overline{s}|s,a)+\xi
\end{equation}

Since we need to ensure that the sum of the probabilities sum to one and that
no single transition probability falls below zero or exceeds one we are only
allowed to modify the probability distribution by at most $\xi$, as given by
equation \eqref{equation:xi}. 

\begin{equation}
\label{equation:xi}
\xi = \min\{
  1 - \tilde{P}(\overline{s} | s, a)
  , \tilde{P}(\underline{s} | s, a)
  , \Delta \omega 
\}
\end{equation}

\begin{equation}
  \Delta \omega = \frac{\sqrt{\frac{2|ln(2^{|S|}-2) - ln  \delta |}{N(s,a)}}}{2}
\end{equation}

$|S|$ denotes the total number of states and $\Delta \omega$ denotes the total
probability mass to be moved. If $\xi$ is less than $\Delta \omega$, new states
$\overline{s}$ and $\underline{s}$ are found, and probabilities moved until
mass equal to $\Delta \omega$ has been moved in total. 

The confidence interval $CI(\hat{P} | N(s, a), \delta_1)$
(equation~\eqref{equation:balls_of_steels}) denotes a set of probability
distributions where the probability is $1 - \delta$ for each element in that
set where the confidence interval is within distance of $\omega(N(s,a),\delta)$
of the maximum likelihood estimate for P \parencite{dietterich2013pac}. This
means that with probability $1-\delta$, the actual transition probabilties are
between the observed probabilities and the probabilities returned by
Compute$\tilde{P}$.

\label{goto}
