\subsection{Our contribution}

\paragraph{Optimizations based on Good-Turing estimations}

\label{sec:mbie_gt}

One problem with the method described above is that probability mass can be
moved to any outcome state, without any consideration taken as to whether this
outcome has ever been observed. \textcite{dietterich2013pac} make
use of an optimization that deals with this by limiting the probability mass
that can be moved to outcomes that have never been observed. The limit that is used is the approximation
of the probability mass in unobserved outcomes as estimated by Good and Turing
as $\hat{M}_0(s,a) = |N_1(s,a)| / N(s,a)$ \parencite{gtpaper}. In this
equation $N_1(s,a)$ is a set of the states that have been observed exactly
once as an outcome when taking action $a$ in state $s$ and $N(s,a)$ is the
number of times that action $a$ has been taken in state $s$ in total. 

\paragraph{Optimizing bounds} Another optimization that can be performed is that the value $\Delta \omega$ in
equation~\eqref{equation:xi} can be tweaked to fit the environment that the agent
is used with. Equation \eqref{equation:xi} gives bounds for which it can be
proved that the method always converges to an optimal policy. In practice,
however, this value can be reduced by quite a bit in order to speed up the rate
at which the agent considers state-action pairs known. 

A simple linearly declining function can be used instead of
equation~\eqref{equation:xi}. In the so called realistic implementation of MBIE we have
used $\omega = 1 - \alpha N(s,a).$ The value of the $\alpha$ parameter was decided through experimentation (see section \ref{sec:test_spec}).
