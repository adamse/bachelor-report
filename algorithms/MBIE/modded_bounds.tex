\subsection{Implementation choices and extensions for MBIE}
\label{sec:MBIE_our_contribution}

\paragraph{How often to perform planning}
\label{sec:mbie_perform_planning}

It is possible to perform planning and compute a new policy once for each
action taken by the agent. However, this would be unnecessarily slow to
compute. The planning comprises iterating Q-value updates to convergence and
then using these converged values to update V-tables, a considerable number of
computations. So instead of planning after every action taken, the algorithm
only performs planning and updates the policy at some given interval. 

For small variants of the Invasive Species environment the policy computation
is performed every time the number of observations of a state-action pair has
doubled. For large variants we perform planning when the total number of
actions taken has increased by 1.5. A large variant is defined as when the
number of state variables exceeds 9, this number was determined by running some
preliminary tests and choosing a value giving a reasonable run time.

\paragraph{Optimizing bounds}

Another optimization that can be performed is tweaking $\Delta \omega$ in
equation~\eqref{equation:xi} to fit the environment that the agent is used
with. Equation~\eqref{equation:xi} gives bounds for which it can be proved that
the method converges to an optimal policy, given some confidence parameter. In
practice, however, this value can be reduced by quite a bit in order to speed
up the rate at which the agent considers state-action pairs known. 

A simple linearly declining function can be used instead of
equation~\eqref{equation:xi}. In the so called realistic implementation of MBIE
we have used $\omega = 1 - \alpha N(s,a).$ The value of the $\alpha$ parameter
was decided through experimentation (see section \ref{sec:test_spec}).

