\subsection{Implementation choices and extensions for MBIE}
\label{sec:MBIE_our_contribution}

\paragraph{How often to perform planning}
\label{sec:mbie_perform_planning}

It is possible to perform planning and compute a new policy once for each
action taken by the agent. However, this would be unnecessarily slow to
compute. The planning comprises iterating Q-value updates to convergence and
then using these converged values to update V-tables, a considerable number of
computations. So instead of planning after every action taken, the algorithm
only performs planning and updates the policy at some given interval. 

A way to do this that we have used is to only perform an update when the number 
of visits to a specific state-action pair has doubled. This is
done for small variants of the Invasive Species environment. For large variants
we perform planning when the total number of actions taken has been multiplied
by 1.5. A large variant is defined as when the number of state variables
exceeds 9. This number was determined by running some preliminary tests and
choosing a number giving a reasonable run time.

\paragraph{Optimizing bounds}

Another optimization that can be performed is that the value $\Delta \omega$ in
equation~\eqref{equation:xi} can be tweaked to fit the environment that the
agent is used with. Equation \eqref{equation:xi} gives bounds for which it can
be proved that the method converges to an optimal policy, given some confidence parameter. In practice,
however, this value can be reduced by quite a bit in order to speed up the rate
at which the agent considers state-action pairs known. 

A simple linearly declining function can be used instead of
equation~\eqref{equation:xi}. In the so called realistic implementation of MBIE we have
used $\omega = 1 - \alpha N(s,a).$ The value of the $\alpha$ parameter was decided through experimentation (see section \ref{sec:test_spec}).

