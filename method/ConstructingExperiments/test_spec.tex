\section{Test specification}
\label{sec:test_spec}

The testing of the agents required us to choose certain sets of parameters, for
the environment, the two different agents and the experiment itself. 

\paragraph{Environment parameters}

The Invasive Species environment requires a number of parameters. For further
explanation of the environment parameters consult the environment
webpage\footnote{\url{http://2013.rl-competition.org/domains/invasive-species}}.
The parameters used in testing the agents were as follows.

\begin{table}[H]
\centering
\captionof{table}{Dynamic parameters common to both species}
\label{tab:dynamic_params_common} 
\begin{tabular}{l r}
 \toprule
 Parameter & Value \\
 \midrule
 Eradication rate & 0.85 \\
 Restoration rate & 0.65 \\
 Downstream spread rate & 0.5 \\
 Upstream spread rate & 0.1 \\
 \bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\captionof{table}{Dynamic parameters, for the two species of trees} \label{tab:dynamic_params_not_common}
\begin{tabular}{lrr}
\toprule
 Parameter & Native & Tamarix \\
 \midrule
 Death rate & 0.2 & 0.2 \\
 Production rate & 200 & 200 \\
 Exogenous arrival & Yes & Yes \\
 Exogenous arrival probability & 0.1 & 0.1 \\
 Exogenous arrival number & 150 & 150 \\
 \bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\captionof{table}{Cost function parameters} \label{tab:cost_params} 
\begin{tabular}{lr}
 \toprule
 Parameter & Value \\
 \midrule
 Cost per invaded reach & 10 \\
 Cost per tree & 0.1 \\
 Cost per empty slot & 0.01 \\
 Eradication cost & 0.5 \\
 Restoration cost & 0.9 \\
 \bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
 \captionof{table}{Variable costs depending on number of habitats affected by action}
 \begin{tabular}{lr}
 \toprule
 Parameter & Value \\
 \midrule
 Eradication cost & 0.4 \\
 Restoration cost for empty slot & 0.4 \\
 Restoration cost for invaded slot & 0.8 \\
 \bottomrule
\end{tabular}
\label{tab:cost_params_var}
\end{table}

\paragraph{Agent Parameters}

The agents evaluated required different types of parameters. Some preliminary
tests were run and then the parameters giving best results were chosen.
Parameters for MBIE are found in table~\ref{tab:mbie_params} and
table~\ref{tab:mbie_realistic_params}.

\begin{table}[H]
    \centering
    \captionof{table}{Parameters for proper MBIE}
    \label{tab:mbie_params} 
    \begin{tabular}{lr}
     \toprule
     Parameter & Value \\
     \midrule
     Discount factor, $\gamma$ & 0.9 \\
     Confidence, $\delta$ & 95\% \\
     $\Delta \omega$ & $\frac{1}{2}\sqrt{\frac{2|\ln(2^{|S|}-2) - \ln  \delta |}{N(s,a)}}$ \\
     
     \bottomrule
    \end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \captionof{table}{Parameters for realistic MBIE}
    \label{tab:mbie_realistic_params}
    \begin{tabular}{lr}
     \toprule
     Parameter & Value \\
     \midrule
     Discount factor, $\gamma$ & 0.9 \\
     $\Delta \omega$ & $max(0,1 - 0.05 N(s,a)$) \\
     \bottomrule
    \end{tabular}
\end{table}

For DBN-\etre\ a higher exploration limit resulted in the agent starting to
exploit earlier but with a slightly lower final return. A higher partial state
known limit resulted in later exploitation but no appreciable difference in
final return. The values in table~\ref{tab:dbne3_params} were a good middle
ground.

\begin{table}[H]
\captionof{table}{DBN-\etre\ parameters}
\label{tab:dbne3_params}
\centering
\begin{tabular}{lr}
 \toprule
 Parameter & Value \\
 \midrule
 Discount factor, $\gamma$ & 0.9 \\
 Exploration limit & 5\% \\
 Partial state known limit & 5 \\
 \bottomrule
\end{tabular}
\end{table}

\paragraph{Experiment parameters}

The tests performed had to be long enough to sample enough data to extract
relevant results without making the running time too long. A single test
consisted of a specific number of episodes with a specific length. A good
combination was required to efficiently evaluate the agents. If a single
episode consisted of too many samples it would be difficult to see the learning
process as results are reported as total reward over an episode and that
process might be hidden as its impact on the total reward is smaller with a
longer episode. On the other hand, if an episode length is too short it would
end before the agents could do any valuable learning.

In addition to a satisfactory episode length, a reasonable number of episodes
needs to be sampled for it to be possible to draw conclusions from the results.
If the number of episodes is too small, the convergence of the agents cannot be
seen. However, too many episodes would lead to unecessary data collection due
that the algorithms would already have converged and no interesting changes
would happen. Some preliminary experiments were run in order to tune the
experiment parameters to suitable values. The episode length was set to 100
samples and there were 100 episodes per test.

To evaluate the agents in the Invasive Species environment combinations of
reaches and number of habitats per reach that can be seen in
table~\ref{tab:reaches_habitats} were chosen. Combinations were chosen to have
a wide range in the total number of states for the agents to deal with and to
test how the agents deal with taking actions that have to take into account
several state components. 

As seen in table ~\ref{tab:reaches_habitats}, the state count increases rapidly
when habitats are added to the problem. This is obvious from the fact that the
state count depends exponentially on the total number of habitats; see equation
\eqref{eq:statecount}, where $h$ is the number of habitats per reach and $r$ is
the number of reaches.

\begin{equation}
\label{eq:statecount}
 |S| = 3 ^ {hr}
\end{equation}

\begin{table}[H]
\centering
\caption{Combinations of reaches and habitats used in testing.}
\label{tab:reaches_habitats}
\begin{tabular}{rrr}
 \toprule
 Reaches & Habitats per reach & Total state count \\
 \midrule
 5  & 1 &          243 \\
 3  & 2 &          729 \\
 3  & 3 &      19\,683 \\
 10 & 1 &      59\,049 \\
 4  & 3 &     531\,441 \\
 5  & 3 & 14\,348\,907 \\
 \bottomrule
\end{tabular}
\end{table}
