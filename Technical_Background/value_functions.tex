\section{Basic algorithms for solving MDPs}

A policy $\pi$ is a function from a state $s$ to an action $a$ that operates in
the context of a Markov decision process, i.e. $\pi \colon S \to A$. A policy
is thus a description of how to act in each state of an MDP. An arbitrary
policy is denoted by $\pi$ and the optimal policy (the policy with the largest
expected reward in the MDP) is denoted by $\pi^*$. The rest of this section
describes some basic algorithms for solving, that is to say finding an optimal
policy, in an MDP \parencite{barto1998reinforcement}.

\subsection{Value functions}

To solve an MDP most algorithms use an estimation of values of states or
actions. Two value functions are usually defined, the state-value function $V :
S \to \mathbb R$ and the state-action-value function $Q : S \times A \to
\mathbb R$. As the names imply, $V$ signifies how good a state is, while $Q$
signifies how good an action in a state is. The state-value function,
$V^\pi(s)$, returns the expected value when starting in state $s$ and following
policy $\pi$ thereafter. The state-action-value function $Q^\pi(s, a)$ returns
the expected value when starting in state $s$ and taking action $a$ and
thereafter following policy $\pi$. The value functions for the optimal policy
are denoted by $V^*(s)$ and $Q^*(s, a)$. Formulas for the state-action value
functions are analogous to those presented for the state value functions
\parencite{barto1998reinforcement}. 

Equations \eqref{equation:v_finite} and \eqref{equation:v_infinite} show the
equations for the state-value function for a finite time horizon and an
infinite, discounted, time horizon respectively. The future states, $s_t$, 
are stochastically distributed according to the transition probabilities
of the MDP (see section~\ref{sec:mdps}).

\begin{align}
\label{equation:v_finite}
V_t^\pi(s) = \mathbb{E} \left\{
  \left. \sum^{T}_{k=0} R(s_{t+k+1})
  \right\vert s_t = s, \pi
\right\}
\end{align}

\begin{align}
\label{equation:v_infinite}
V_{t}^\pi(s) = \mathbb{E} \left\{
  \left. \sum_{k=t}^\infty \gamma^k R(s_{t+k+1})
  \right\vert s_t = s,\pi
\right\}
\end{align}

Both $V^\pi$ and $Q^\pi$ can be estimated from experience. This can be done by
maintaining the average of the rewards that have followed each state when
following the policy $\pi$. When the number of times the state has been
encountered goes to infinity, the average over these histories of rewards
converges to the true values of the value function
\parencite{barto1998reinforcement}.

\input{Technical_Background/ValueFunctions/dynamic_programming.tex}
\input{Technical_Background/ValueFunctions/policy_iteration.tex}
\input{Technical_Background/ValueFunctions/value_iteration.tex}
