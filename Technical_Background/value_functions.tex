\section{Basic algorithms for solving MDPs}

A policy $\pi$ is a function from a state $s$ to an action $a$ that operates in
the context of a Markov Decision Process, i.e. $\pi \colon S \to A$. A policy
is thus a description of how to act in each state of an MDP. An arbitrary
policy is denoted by $\pi$ and the optimal policy (the policy with the largest
expected reward in the MDP) is denoted by $*$. The rest of this section is
describes some basic algorithms for solving, that is to say finding an optimal
policy, in an MDP.

\subsection{Value functions}

To solve an MDP most algorithms uses an estimation of values of states or
actions. Two value functions are usually defined, the state-value function $V :
S \to \mathbb R$ and the state-action-value function $Q : S \times A \to
\mathbb R$. As the names implies $V$ signifies how good a state is, while $Q$
signifies how good an action in a state is. The state-value function,
$V^\pi(s)$, returns the expected value when starting in state $s$ and following
policy $\pi$ thereafter. Equations \eqref{equation:v_finite} and \eqref{equation:v_infinite} show the equations for the state-value function for a finite time horizon and an infinite, discounted, time horizon respectively. The state-action-value function $Q^\pi(s, a)$ returns
the expected value when starting in state $s$ and taking action $a$ and
thereafter following policy $\pi$. The value functions for the optimal policy
are denoted by $V^*(s)$ and $Q^*(s, a)$. Formulas for the state-action value functions are anologous to those presented for the state value functions. 

\begin{align}
\label{equation:v_finite}
V_t^\pi(s) = \mathbb{E} \{ \sum_{k=0}^{T=k} r_{t+k} | S_t = S, \pi \}
\end{align}

\begin{align}
\label{equation:v_infinite}
V_{0}^\pi(s) = \mathbb{E} \{ \sum_{t=0}^\infty r_{t}\gamma^t | S_0 = S,\pi \}
\end{align}

Both $V^\pi$ and $Q^\pi$ can be estimated from experience. This can be done by
maintaining the average of the rewards that have followed each state when
following the policy $\pi$. When the number of times the state has been
encountered goes to infinity, the average over these histories of rewards
converges to the true values of the value function
\parencite{barto1998reinforcement}.

\input{technical_background/ValueFunctions/dynamic_programming.tex}

\input{technical_background/ValueFunctions/policy_iteration.tex}

\input{technical_background/ValueFunctions/value_iteration.tex}
