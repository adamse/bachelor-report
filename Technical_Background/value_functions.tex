\section{Basic algorithms for solving MDPs}

A policy $\pi$ is a function from a state $s$ to an action $a$ that operates in
the context of a Markov decision process, i.e. $\pi \colon S \to A$. A policy
is thus a description of how to act in each state of an MDP. An arbitrary
policy is denoted by $\pi$ and the optimal policy (the policy with the largest
expected reward in the MDP) is denoted by $\pi^*$. The rest of this section
describes some basic algorithms for solving, that is to say finding an optimal
policy, in an MDP \parencite{barto1998reinforcement}.

\subsection{Value functions}

To solve an MDP most algorithms use an estimation of values of states or
actions. Two value functions are usually defined, the state-value function $V :
S \to \mathbb R$ and the state-action-value function $Q : S \times A \to
\mathbb R$. As the names imply, $V$ signifies how good a state is, while $Q$
signifies how good an action in a state is. The state-value function,
$V^\pi(s)$, returns the expected value when starting in state $s$ and following
policy $\pi$ thereafter. Equations \eqref{equation:v_finite} and \eqref{equation:v_infinite} show the equations for the state-value function for a finite time horizon and an infinite, discounted, time horizon respectively. In \eqref{equation:v_infinite}, $\gamma$ is the discount factor, a value between 0 and 1 which describes how fast the value of expected future rewards decay as one looks further into the future. The state-action-value function $Q^\pi(s, a)$ returns
the expected value when starting in state $s$ and taking action $a$ and
thereafter following policy $\pi$. The value functions for the optimal policy
are denoted by $V^*(s)$ and $Q^*(s, a)$. Formulas for the state-action value functions are analogous to those presented for the state value functions \parencite{barto1998reinforcement}. 

\begin{align}
\label{equation:v_finite}
V_t^\pi(s) = \mathbb{E} \left\{\left. \sum^{T}_{k=0} r_{t+k} \right\vert s_t = s, \pi \right\}
\end{align}

\begin{align}
\label{equation:v_infinite}
V_{0}^\pi(s) = \mathbb{E} \left\{\left. \sum_{t=0}^\infty r_{t}\gamma^t \right\vert s_0 = s,\pi \right\}
\end{align}

Both $V^\pi$ and $Q^\pi$ can be estimated from experience. This can be done by
maintaining the average of the rewards that have followed each state when
following the policy $\pi$. When the number of times the state has been
encountered goes to infinity, the average over these histories of rewards
converges to the true values of the value function
\parencite{barto1998reinforcement}.

\input{Technical_Background/ValueFunctions/dynamic_programming.tex}
\input{Technical_Background/ValueFunctions/policy_iteration.tex}
\input{Technical_Background/ValueFunctions/value_iteration.tex}
