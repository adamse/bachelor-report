\section{Basic algorithms for solving MDPs}

A policy $\pi$ is a function from a state $s$ to an action $a$ that operates in
the context of a Markov decision process, i.e. $\pi \colon S \to A$. A policy
is thus a description of how to act in each state of an MDP. An arbitrary
policy is denoted by $\pi$ and the optimal policy (the policy with the maximal
utility in the MDP) is denoted by $\pi^*$. The rest of this section
describes some basic algorithms for solving an MDP, that is to say, finding an optimal
policy for it \parencite{barto1998reinforcement}.

\subsection{Value functions}

To solve an MDP, most algorithms use an estimation of values of states or
states and actions. Two value functions are usually defined, the state-value function $V :
S \to \mathbb R$ and the state-action-value function $Q : S \times A \to
\mathbb R$. As the names imply, $V$ signifies how good a state is, while $Q$
signifies how good an action in a state is. The state-value function,
$V^\pi(s)$, returns the expected value when starting in state $s$ and following
policy $\pi$ thereafter. The state-action-value function, $Q^\pi(s, a)$, returns
the expected value when starting in state $s$ and taking action $a$ and
thereafter following policy $\pi$. The value functions for the optimal policy, the policy
that maximizes the utility, 
are denoted by $V^*(s)$ and $Q^*(s, a)$ \parencite{barto1998reinforcement}. 

Equations \eqref{equation:v} and \eqref{equation:q} show the state-value
function $V$ and the state-action-value function $Q$ defined in terms of
utility (section~\ref{sec:utility}). The future states, $s_t$, are
stochastically distributed according to the transition probabilities of the MDP
and the policy $\pi$ (see section~\ref{sec:mdps}).

\begin{align}
\label{equation:v}
V_t^\pi(s) = \mathbb{E} \left\{
  \left. U_t
  \right\vert s_t = s
\right\}
\end{align}

\begin{align}
\label{equation:q}
Q_t^\pi(s, a) = \mathbb{E} \left\{
  \left. U_t
  \right\vert s_t = s, a_t = a
\right\}
\end{align}

Both $V^\pi$ and $Q^\pi$ can be estimated from experience. This can be done by
maintaining the average of the rewards that have followed each state when
following the policy $\pi$. When the number of times the state has been
encountered goes to infinity, the average over these histories of rewards
converges to the true values of the value function
\parencite{barto1998reinforcement}.

\paragraph{Using dynamic programming to find $V$ and $Q$}

Another way to find value functions is to use dynamic programming techniques. Dynamic programming is a way of dividing a problem into subproblems that can be
solved independently. If the result of a particular subproblem is needed again,
it can be looked up from a table \parencite{bellman1957mdp}. Examples of dynamic programming algorithms are
policy iteration and value iteration, which are discussed in sections
\ref{sec:pol_itr} and \ref{sec:valueiteration}. These algorithms are often the
basis for more advanced algorithms, among them the ones described in chapter
\ref{ch:algo}. 

\input{Technical_Background/policy_iteration.tex}
\input{Technical_Background/value_iteration.tex}
