\subsection{Value iteration}
\label{sec:valueiteration}

Value iteration is a simplification of policy iteration where only one step of
policy evaluation is performed in each iteration. Value iteration does not compute an actual
policy until the value function has converged \parencite{barto1998reinforcement}. Value iteration works as follows:

\begin{description}
\item[Initialization] \hfill \\
    Start with an arbitrary value function $V_0$.
\item[Value iteration] \hfill \\
    Update the value function for each state using the update rule \eqref{eq:valiter}.

\begin{equation} \label{eq:valiter}
V_{k+1}(s) = \max_a \sum_{s'}{P(s, a, s') \left[R(s, a) + \gamma V_k(s')\right]}
\end{equation}

\item Repeat value iteration until $V$ converges and set $V = V_k$. As in policy iteration, by
  convergence is meant that $|V_{k+1}(s) - V_{k}(s)| \leq \epsilon, \forall s$, where
  $\epsilon$ is a small value. 

\item Compute the policy using equation ~\eqref{eq:valiterpolicy}.

\begin{equation} \label{eq:valiterpolicy}
\pi(s) = \operatorname*{arg\,max}_a \sum_{s'}{P(s, a, s') \left[R(s, a) + \gamma V(s')\right]}
\end{equation}

\end{description}
