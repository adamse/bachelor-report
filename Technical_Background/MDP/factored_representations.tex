\subsection{Representations \note{Ny text}}
Two ways to separate the representations of an MDP is extensional or factored representation. Depending on the problem domain it can be advantageous from the computational view to use factored representation \parencite{dean1999descision}.

\paragraph{Extensional Representation \note{Ny Text}}
The most straightforward way to model an MDP is called Extensional representation, where the set of states and actions are enumerated directly. It is also commonly refered to as an explicit representation and closely mirrors the definition we have used so far in the report when discussing the abstract view of an MDP \parencite{dean1999descision}.

\paragraph{Factored Representation \note{Ny Text}} 
A factored representation of the states of an MDP often results in an more compact way of describing the set of states. Certain properties or features of the states are used to categorize the states into different kind of sets. Then one can treat all the members of the same set in the same manner. Which properties or features are used is chosen by the algorithm designer, to fit the environment to be used. 

When the MDP is factored, it enables a factored representation of rewards, actions and other components of the MDP as well. When using a factored action representation, an action can be taken based on specific state features instead of on the whole state. If the individual actions affect relativity few features or if the effects contain regularities, using a factored representation often results in compact representations of actions \parencite{dean1999descision}. 