\paragraph{Transition Functions}

Furthermore, an MDP requires a transition function $P(s, a, s')$ which describes the probability that the next state will be $s'$ if action $a$ is taken in state $s$. This means that the future state $s'$ depends on the current state s and the action taken, but not any previous transitions. Another way of expressing this is saying that the Markov property holds for an MDP. \parencite{altman2002applications}.

%Borttagen ur dokumentet. Anses on√∂dig.