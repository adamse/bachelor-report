\section{Markov decision process}
\label{sec:mdps}

Within reinforcement learning, the concept of Markov decision processes (MDP) is central. An MDP is a way to model an environment where state changes in it are dependent on both random chance and the actions of an agent. An MDP is defined by the quadruple $\left( S, A, P( \cdot , \cdot, \cdot ) , R( \cdot , \cdot ) \right)$ \parencite{altman2002applications}:

\begin{definition}{MDP}

$S$ \quad A set of states that the environment can be in.

$A$ \quad A set of actions that are allowed in the MDP.

$P \colon S \times A \times S \to \mathbb \lbrack0, 1\rbrack$ \quad
    A probability distribution over the transitions in the environment. This
    function describes the probability of ending up in a certain target state
    when a certain action is taken from a certain origin state. 

$R \colon S \times A \to \mathbb{R}$ \quad
    A function for the reward associated with a state transition. In some
    definitions of MDPs the reward function only depends on the state.
\end{definition}

MDPs are similar to Markov chains, but there are two differences. First, there
is a concept of rewards in MDPs, which is absent in Markov chains. Second, in a
Markov chain, the only thing that affects the probabilities of transitioning to
other states is the current state, whereas in an MPD both the current state and
the action taken in that state are needed to know the probability distribution
connected with the next state \parencite{altman2002applications}.

If there are only a few states $s'$ for which $P(s, a, s') > 0$, the MDP is called a sparse MDP. That is to say, when an agent performs a certain action, $a$, in a certain state, $s$, the environment can only end up in a small fraction out of the total number of states \parencite{dietterich2013pac}. 

\subsection{Utility}
\label{theBestLabelEvah}
\label{sec:utility}


The concept of utility is used as a measurement of how well an agent performs
over time. In the simplest case, the utility is just the sum of all rewards
received in an episode. See equation \eqref{equation:utilNondisc}, where $T$ is the
length of an episode and $r_k$ is the expected reward at time step $k$.

\begin{equation}
\label{equation:utilNondisc}
U_0 = r_{0} + r_{1} + \dots + r_{T-1}
  = \sum\limits_{k = 0}^{T-1} r_{k}
\end{equation}

However, in a non-episodic environment, the interaction between the environment and the agent can go on forever.
Here the concept of a discount factor, $\gamma$, is essential. This parameter
is a value between 0 and 1 which describes how fast the value of expected
future rewards decays as one looks further into the future
\parencite{barto1998reinforcement}. Thus, the utility can be expressed as in
equation \eqref{equation:utilDisc} in the case that discounting is used. 

\begin{equation}
\label{equation:utilDisc}
U_{t} = r_{t + 1} + \gamma r_{t + 2} + \gamma^2 r_{t + 3} \dots
  =  \sum\limits_{k = 0}^\infty \gamma^kr_{t+k+1}
\end{equation}



\input{Technical_Background/MDP/markov_property.tex}
%\input{Technical_Background/MDP/sparse_mdps.tex}
\input{Technical_Background/MDP/factored_representations.tex}
