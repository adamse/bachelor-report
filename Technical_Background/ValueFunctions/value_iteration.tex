\subsection{Value iteration}
\label{sec:valueiteration}

Value iteration is a simplification of policy iteration where only one step of policy evaluation is performed in each iteration \parencite{barto1998reinforcement}. Value iteration does not compute an actual policy until the value function has converged. Value iteration works as follows:

\begin{description}
\item[Initialization] \hfill \\
    Start with an arbitrary value function $V$.
\item[Value iteration] \hfill \\
    Update $V$ for each state using the update rule~\eqref{eq:valiter}
\item Repeat value iteration until $V$ converges
\item Compute the policy using~\eqref{eq:valiterpolicy}
\end{description}

\begin{equation} \label{eq:valiter}
V_{k+1}(s) = \max_a \sum_{s'}{P(s, a, s') \left[R(s, a) + \gamma V_k(s')\right]}
\end{equation}

\begin{equation} \label{eq:valiterpolicy}
\pi(s) = \operatorname*{arg\,max}_a \sum_{s'}{P(s, a, s') \left[R(s, a) + \gamma V_k(s')\right]}
\end{equation}

%If the policy evaluation of the Policy Iteration would stop after just one sweep, this the would correspond to the algorithm Value Iteration.
\begin{comment}
\begin{algorithm}

\caption{High-Level Description of Value Iteration}          
\label{value_iteration_high_level}

\begin{algorithmic}
\State Initialize $V$ arbitrarily for all $s \in S$
\Loop
    \For{every state $s$}
        \State Update $V(s)$ with: The action which generates highest sum of (the probability that the next state will be s’, due to each of the allowed actions, multiplied with the addition of the expected reward and V(s')).
        \If{The value function only changed more than a small value} 
            \State Go to Repeat.
        \EndIf
    \EndFor
\EndLoop
\end{algorithmic}
\end{algorithm}
\end{comment}
\begin{comment}

Initialization
	Repeat
	For every state s
		Update V(s) with: The action which generates highest sum of (the probability that the next state will be s’, due to each of the allowed actions, multiplied with the addition of the expected reward and V(s’)). 
	If the value function only changed more than a small value then go to Repeat.


In each sweep, Value Iteration does both policy evaluation and policy improvement. Value Iteration converges to an optimal policy for discounted finite MDPs, but would require an infinite number of iterations to converge exactly to V*. On implementation, you would stop the algorithm when the Value function only changes by a small amount, an amount of your choosing. 
\parencite{barto1998reinforcement}
\end{comment}
