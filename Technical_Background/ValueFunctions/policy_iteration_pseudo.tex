\begin{algorithm}                                        

\caption{High-Level Description of Policy Iteration}
\label{policy_iteration_high_level}

\begin{algorithmic}
    %\State Initialization THIS LINE FUCKS EVERYTHING UP, DO NOT UNCOMMENT IDIOT
    \State Choose initial value function $V(s)$, policy, $\pi$, arbitrarily
    \State Policy Evaluation
    \For{every state $s$}
        \State Save in variable $v$ $V(s)$
        \State Update $V(s)$: The sum of (the probability that 
        the next state will be $s'$, due to the policy, multiplied
        with the addition of the expected reward and $V(s'))$. 
	    \State Repeat Policy Evaluation until the largest change is small enough.  
    \EndFor
    \State Policy improvement
    
\end{algorithmic}
\end{algorithm}

\begin{comment}
Initialization
    Choose your initial value function, V(s)
    and policy, pi arbitrarily
Policy Evaluation
  For every state s:
    Save in variable v V(s)
    Update V(s) with: The sum of (the probability that 
    the next state will be s’, due to the policy, multiplied
    with the addition of the expected reward and V(s’)). 
	Repeat Policy Evaluation until the largest change is small enough.  
Policy Improvement
	For every state s: 
		Update the policy of that state the action which generate the most 
		expected reward + value function.
\end{comment}