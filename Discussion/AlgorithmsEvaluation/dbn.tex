\subsection{\etre\ in Factored Markov Decision Processes 
\note{N}}
\label{sec:e3_factored_discussion}

\paragraph{General shape of the results} In a comparison of the \etre\ agent's behavior in the different-size environments, the similarity of the shape of the graphs is striking. At first there is a period of lower and lower performance. (For the smallest environments, this period is very short, however.) Next, there is a period of fairly constant high performance, which lasts until the end of the experiment.  

This shape can be explained as a consequence of the different phases of operation of the \etre\ algorithm coupled with the structure of the MDP studied. In the beginning of the experiment, the algorithm will spend almost all of its time in the balanced wandering and exploration phases. The longer the agent has been exploring, and the more states become known, the further the agent has to explore into hard-to-reach parts of the MDP to find unexplored states. Now, in the case of the invasive species environment with the parameters chosen as in the experiments presented, the most easily reachable states are the ones where there is no tamarisk infection. This means that the harder a state is to reach, the more infected it will be, and thus the performance of the \etre\ agent in the exploration phase will fall as the experiment progresses. 

However, once the agent knows enough about the environment to enter the exploitation phase, the \etre\ agent spends close to no time at all exploring unknown states, and it retains high performance. 

\paragraph{Consequences of pooling observation data} An interesting result is that the agent is able to enter the exploitation phase for the 10 reaches/1 habitat per reach test setup slightly earlier than for the 5 reaches/1 habitat per reach setup. This is probably connected to the optimization described in the last paragraph of section \ref{sec:factored_e3}. In the 10 reaches/1 habitat per reach case there are nine reaches with two parents (including the reach itself) and one reach with only itself as parent. In the 5 reaches/1 habitat per reach case there are four reaches with two parents and one reach with only itself as parent. Since observations for state variables with similar parent structure are pooled by our \etre\ implementation, the agent will receive almost double the information per time step for the two-parent reaches in the 10 reaches case as compared to the 5 reaches case.  

\paragraph{Possible issues with the one-policy-per-reach optimization} In section \ref{sec:one_policy_per_state_variable} an optimization is described that works very well for the particular environment and environment settings that the agent was tested for. However, this optimization makes several assumptions that may cause problems if the settings are changed. For instance, one assumption is that the state of a reach is only affected directly by its adjacent parents in river network. If the state of a reach was made to depend significantly on other reaches two or more levels up in the river network, the agent would probably not be able to converge on an optimal policy. 

Another assumption that could lead to problems with other environment settings is the assumption that the maximal action cost in the environment is impossible or very hard to break. The invasive species environment has a maximum cost for actions. However, with the standard settings it is mathematically impossible to break this maximum. Our implementation of \etre\ would achieve very poor performance if this was not the case, since a large penalty is given when the maximum action cost is breached. 

\paragraph{DBN structure} Finally, in the invasive species environment, the structure of the DBN underlying the MDP is known at the start of the experiment, so the agent does not need to infer it from its observations. If this was not the case, all the DBN optimizations would be useless unless some kind of algorithm for infering the DBN structure was added to the agent. 
