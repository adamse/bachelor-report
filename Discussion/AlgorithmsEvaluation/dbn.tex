\subsection{\etre\ in Factored Markov Decision Processes 
\note{I}}
\label{sec:e3_factored_discussion}

In a comparison of the \etre\ agent's behavior in the different-size environments, the similarity of the shape of the graphs is striking. At first there is a period of lower and lower performance. (For the smallest environments, this period is very short, however.) Next, there is a period of fairly constant high performance, which lasts until the end of the experiment.  

This shape can be explained as a consequence of the different phases of operation of the \etre\ algorithm coupled with the structure of the MDP studied. In the beginning of the experiment, the algorithm will spend almost all of its time in the balanced wandering and exploration phases. The longer the agent has been exploring, and the more states become known, the further the agent has to explore into hard-to-reach parts of the MDP to find unexplored states. Now, in the case of the invasive species environment with the parameters chosen as in the experiments presented, the most easily reachable states are the ones where there is no tamarisk infection. This means that the harder a state is to reach, the more infected it will be, and thus the performance of the \etre\ agent in the exploration phase will fall as the experiment progresses. 

However, once the agent knows enough about the environment to enter the exploitation phase, the \etre\ agent spends close to no time at all exploring unknown states, and it retains high performance. 

An interesting result is that the agent is able to enter the exploitation phase for the 10 reaches/1 habitat per reach test setup slightly earlier than for the 5 reaches/1 habitat per reach setup. This is probably connected to the optimization described in the last paragraph of section \ref{sec:factored_e3}. In the 10 reaches/1 habitat per reach case there are nine reaches with two parents (including the reach itself) and one reach with only itself as parent. In the 5 reaches/1 habitat per reach case there are four reaches with two parents and one reach with only itself as parent. Since observations for state variables with similar parent structure are pooled by our \etre\ implementation, the agent will receive almost double the information per time step for the two-parent reaches in the 10 reaches case as compared to the 5 reaches case.  


%Förväntar oss: en period av kassa resultat följt av en period av hyfsat konstant ok resultat. 

%Borde kunna få sämre assymptotiskt beteende för E3, om den bestämmer sig för att sluta explora för tidigt. 

%planning-per-reach och DBN-tjosan gör att 5-1 når exploitation på LÄNGRE tid (fler steg) än 10-1, eftersom både 5-1 och 10-1 (förmodligen) båda innehåller 1, 1-1 och 1-2 pv-reaches, medan 10-1 har flera av varje av dessa typerna. Detta gör att 10-1 får mer information per tidssteg om ett i praktiken lika stort problem som 5-1. 

%Varför i hellllllvete blir MBIE inte lika bra som E3 asymptotiskt´? Det borde inte va nån skillnad. 

%