\subsection{DBN-\etre\ vs MBIE}
\label{sec:dbn_vs_mbie}

An expectation on both agents is that they should converge to a near optimal
behavior as time goes to infinity. However, it is clear from the results
presented in chapter \ref{ch:results} that neither version of the MBIE agent
reaches the same level of performance as the DBN-\etre\ agent in the tests we
performed. In the smaller problem as seen for example in
figure~\ref{fig:tests1}b, the difference is not large between the DBN-\etre\ agent
and the realistic MBIE. Nevertheless, it is still clear that DBN-\etre\ is far
superior in finding an optimal policy.

\textcite{strehl2004empirical} tested the performance of the MBIE algorithm
along with the \etre\ algorithm in two non-factored environments, RiverSvim and
Sixarms. \textcite{strehl2004empirical} concludes that the MBIE outperforms
\etre\ in both of the environments. This thesis utilizes a factored environment
for tests and thus the DBN-\etre\ algorithm outperforms the non-factored MBIE
algorithm. Due to the significant performance increase in relation to the
non-factored version of the \etre\ algorithm the DBN-\etre\ outperforms the
MBIE algorithm.

As discussed in section~\ref{sec:factored_mbie} we did not make any factored
additions to the MBIE algorithm and it is uncertain wether the DBN-\etre\
algorithm would retain its superior performace if this had been done.

\paragraph{Unfair comparisons}

The comparison between our implementations of MBIE and DBN-\etre\ are not very
fair. The DBN-\etre\ implementation has been heavily optimized to work with
factored MDPs and the invasive species environment in particular, whereas the
MBIE implementation is much more generalized. The discussions regarding the
generality of the agents in furher continued in section
\ref{sec:impact_of_one_env}. To make a more nuanced comparison between MBIE and
DBN-\etre\ one could use smaller non-factored environments as well as the
larger factored invasive species environment.


\paragraph{Large state spaces}

When the number of states is increased it is clear that the algorithms still
work. In a comparison between the 4 reaches, 3 habitats-per-reach case seen in
figure~\ref{fig:tests3}a, and the 5 reaches, 3 habitats-per-reach case seen
in figure~\ref{fig:tests2}b, this can be seen. The algorithms still improve
over time although taking longer time to converge, which is expected as the
state space increases in size. 

A noteable difference between the original and realistic versions of MBIE is
that the realistic version has the same level of performance as the original
MBIE in large state spaces whereas in small state spaces the realistic MBIE
is clearly better than the original. Compare figure~\ref{fig:tests1}b with
figure~\ref{fig:tests2}b to see this difference. In small state spaces the
realistic version will know the real value of more states than the original
version, while in large state spaces both versions will prioritize unknown
states.

The DBN-\etre\ algorithm successfully deals with large state spaces. DBN-\etre\
utilizes a factored approach in representing transition probabilities, which
allows the algorithm to learn about the environment more quickly whereas a
non-factored approach would struggle to learn the transition probabilities.
This can be seen in the result where the DBN-\etre\ algorithm quickly knows
enough to start exploiting even in cases such as figure~\ref{fig:5r3h}.
Furthermore, using a factored approach to policy calculation allows quick
policy calculations, making the algorithm fast to run in large state spaces.
