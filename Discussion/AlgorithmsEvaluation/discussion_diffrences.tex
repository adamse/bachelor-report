\subsection{DBN-\etre\ vs MBIE \note{N}}
An expectation on both agents is that they should converge on optimal behavior
as $t$ goes to infinity. However, it is clear from the results presented in
chapter \ref{ch:results} that neither version of the MBIE agent reaches the
same level of performance as the DBN-\etre\ agent. In the smaller problem as
seen for example in figure~\ref{fig:tests1}b the diffrence is not all that
diffrent between DBN-\etre\ agent and the realistic MBIE. 

Nevertheless, it isstill clear that DBN-\etre\ is far superior in finding the converging policy.
Previous studies have been concluded as a comparisson regarding the original
\etre\ algorithm and MBIE, in \textcite{strehl2004empirical} that MBIE
outperforms \etre\ in their testing environments: RiverSwim and SixArms. This raises an intresting theory or observation. Due to the factored representation of DBN-\etre\ and its underlying dynamic Bayesian network the algorithms performance and usability significantly increases. On its own \etre\ shows poor performance and is significantly outperformed in the cited study and by using the factored representation becoming drasticly improved, derving to the conclusion that by using a good model of the problem as stated in the last paragraph of section \ref{sec:e3_factored_discussion} is able to outperform a relative good algorithm as MBIE showed in this study. The drawback as discussed ealier in section(\todo[]{where do we confess?}) it was not able to implement the underlying structure as a dynamic Bayesian network making it impossible to verify this conslusion regarding that a good model outperforms a good algorithm. Along with the fact that in \textcite{strehl2004empirical} the models used did not enable the algorithms to use factored representations.


\paragraph{Unfair comparisons}
In one sense, the comparison between our implementations of MBIE and DBN-\etre\
are not very fair. The DBN-\etre\ implementation has been heavily optimized to
work with factored MDPs and the invasive species environment in particular,
whereas the MBIE implementation is much more generalized. The discussions
regarding the generality of the agents in furher continued in section
\ref{sec:impact_of_one_env}. 

\paragraph{Large state spaces}
When the number of states are increased it is clear that the algorithms still
work. Comparing the 4 reaches and 3 habitats per reach case seen in
figure~\ref{fig:tests3}a with the 5 reaches and 3 habitats per reach case seen
in figure~\ref{fig:tests2}b this can be seen. The algorithms do still improve
over time although taking longer time to converge, which is expected as the
state space increases in size. 

A noteable difference between the original and realistic versions of MBIE is
that the realistic version has the same level of performance as the original
MBIE in larger state spaces whereas in smaller state spaces the realistic MBIE
was clearly better than the original. Compare figure~\ref{fig:tests1}b with
figure~\ref{fig:tests2}b to see this difference. In smaller state spaces the
realistic version will know the real value of more states than the original
version, while in larger state spaces both versions will prioritise unknown
states.

The DBN-\etre\ algorithm successfully deals with large state spaces. DBN-\etre\
utilizes a factored approach in representing transition probabilities, which
allows the algorithm to learn about the environment quicker whereas a
non-factored approach would struggle to learn the transition probabilities.
This can be seen in the result where the DBN-\etre\ algorithm quickly knows
enough to start exploiting even in cases such as figure~\ref{fig:5r3h}.
Furthermore, using a factored approach to policy calculation allows quick
policy calculations making the algorithm fast to run in large state spaces.
