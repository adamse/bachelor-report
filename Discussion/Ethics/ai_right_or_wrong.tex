\subsection{Artificial Intelligence, right or wrong? \note{N}}
\label{sec:hawkins}

One area heavily researched in the artificial intelligence field is the area of autonomous cars. Google reported in 2014, that their research project with autonomous cars had covered 700 000 miles without human intervention \parencite{Urmson2014:Online}. The main goal of autonomous cars is a safer traffic system, a system where the human factor is no longer is a part of the equation. Nevertheless, the question remains, is the human factor really taken out of the equation? For the end user of the cars the answer is obviously yes. However, the human factor has been transformed into a different role and now lies on the engineers constructing the system for the car, making it able to drive without a driver. The need for worry is no longer for people driving under influence or being stressed or tired influencing their driving. Instead the human factor now lies on the people responsible for creating and deploying the system. Developing software is hard, it requires discipline and experience in order to avoid easy mistakes and errors. Nonetheless, the stakes are higher than when usually developing large scale systems, this time the systems were developed to be used by humans. 

Moving the human factor, presents another relevant question lacking a good answer. Imagine a scenario where a collision is unavoidable for the computer driving the car. \textcite{Lin:Online} raises several intresting aspects and scenarios in the article. However, the most releveant is a realistic but uncommon scenario, which vehicle should the car collide with. Forcing the ethical question onto the people responsible for the system. A choice no human would like to be responsible for and in a way turning the driver-less vehicle into target seeking drone. Not every thing becomes better with technology, looking at the opposite case the driver properly do not have the time to think about the situation leaving the outcome to a sheer coincidence.