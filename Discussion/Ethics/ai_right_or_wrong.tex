\subsection{Artificial intelligence, right or wrong?}
\label{sec:hawkins}

One area heavily researched in the artificial intelligence field is 
autonomous cars. Google reported in 2014, that their research project with
autonomous cars had covered 700\,000 miles without human intervention
\parencite{Urmson2014:Online}. The main goal of autonomous cars is a safer
traffic system, a system where the human factor is no longer a part of the
equation. Nevertheless, the question remains, is the human factor really taken
out of the equation? For the end user of the cars the answer is obviously yes.
However, the human factor has been transformed into a different role and now
lies on the engineers constructing the system for the car, making it able to
drive without a driver. The need for worry is no longer for people driving
under influence or being stressed or tired influencing their driving. Instead
the human factor now lies on the people responsible for creating and deploying
the system. Developing software is hard, it requires discipline and experience
in order to avoid mistakes and errors. Nonetheless, the stakes are higher
than when usually developing large scale systems, this time the systems were
developed to be used by humans.

Several interesting aspects and scenarios are raised in \textcite{Lin:Online}.
A realistic but uncomming scenario is where a collision is unavoidable for a
computer driving a car. This forces the ethical question onto the people
responsible for the system. A choice no human would like to be responsible for
and in a way turning the driver-less vehicle into target seeking drone. Not
everything becomes better with technology, looking at the opposite case the
driver properly do not have the time to think about the situation leaving the
outcome to coincidence while a computer makes an active choice.
